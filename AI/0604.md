- LLM.pdf

### ncp

NCP(Naver Cloud Platform)ëŠ” AWSì²˜ëŸ¼ ì„œë²„, ë°ì´í„°ë² ì´ìŠ¤, ìŠ¤í† ë¦¬ì§€ë¥¼ í´ë¼ìš°ë“œì—ì„œ **ì—¬ëŸ¬ ì‚¬ìš©ìê°€ ë™ì‹œì— ì ‘ê·¼í•˜ê³  ì“¸ ìˆ˜ ìˆëŠ” ì¸í”„ë¼ ì„œë¹„ìŠ¤**

### 1. **LangGraph (ë­ê·¸ë˜í”„)** â€” **AI/LLM ê´€ë ¨ í”„ë ˆì„ì›Œí¬**

**LangGraph**ëŠ” ìµœê·¼ ì£¼ëª©ë°›ëŠ” **LLM(ëŒ€í˜•ì–¸ì–´ëª¨ë¸)** ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì„¤ê³„í•˜ê¸° ìœ„í•œ **ìƒíƒœ ê¸°ê³„ ê¸°ë°˜ í”„ë ˆì„ì›Œí¬**ì…ë‹ˆë‹¤.

LangChainê³¼ í˜¸í™˜ë˜ë©°, ì—ì´ì „íŠ¸ë‚˜ ì›Œí¬í”Œë¡œìš°ë¥¼ **ê·¸ë˜í”„ êµ¬ì¡°**ë¡œ í‘œí˜„í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

### ğŸ”¹ íŠ¹ì§•

- LLMì˜ íë¦„(ì˜ˆ: ì—¬ëŸ¬ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì—ì´ì „íŠ¸)ì„ ìƒíƒœ ê·¸ë˜í”„ë¡œ êµ¬ì„±
- ì¡°ê±´ ë¶„ê¸°, ë£¨í”„ ë“± ë³µì¡í•œ ë…¼ë¦¬ íë¦„ ì„¤ê³„ ê°€ëŠ¥
- LangChain, OpenAI, HuggingFace ë“±ê³¼ ì—°ë™ë¨
- Pythonê³¼ TypeScript ì§€ì›

workspace/ai_agent_work

ai_agent_work/

- gpt_basic.py

```python
from openai import OpenAI
from http import client

client = OpenAI(api_key=api_key)

response = client.chat.completions.create(
    model='gpt-4o',
    temperature=0.1, # ììœ ë„
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "2022ë…„ ì›”ë“œì»µ ìš°ìŠ¹íŒ€ì€ ì–´ë””ì•¼?"},
    ]
)

print(response)
print('------------------')
print(response.choices[0].message.content)

"""
temperatureë€?
ë‚®ì€ ê°’ (ì˜ˆ: 0.0 ~ 0.3)
â†’ ë” ê²°ì •ì (deterministic) ê²°ê³¼
â†’ ì •í™•í•˜ê³  ì¼ê´€ëœ ë‹µë³€ì„ ìƒì„±í•˜ë ¤ í•  ë•Œ ìœ ìš©
â†’ í•­ìƒ ë¹„ìŠ·í•œ ë‹µë³€ì„ ëŒë ¤ì¤Œ

ë†’ì€ ê°’ (ì˜ˆ: 0.7 ~ 1.0)
â†’ ë” ì°½ì˜ì ì´ê³  ë‹¤ì–‘ì„± ìˆëŠ” ì¶œë ¥
â†’ ê¸€ì“°ê¸°, ìŠ¤í† ë¦¬ ìƒì„±, ì•„ì´ë””ì–´ ë¸Œë ˆì¸ìŠ¤í† ë°ì— ì í•©
â†’ ë˜‘ê°™ì€ ì§ˆë¬¸ì—ë„ ë‹¤ì–‘í•œ ë‹µë³€ì„ ìƒì„±

ì¤‘ê°„ ê°’ (ì˜ˆ: 0.5)
â†’ ì ë‹¹í•œ ê· í˜•
"""
```

- gpt_basic2.ipynb

```python
from openai import OpenAI
from http import client
from dotenv import load_dotenv
import os

load_dotenv()

api_key = os.getenv('OPENAI_API_KEY')

client = OpenAI(api_key=api_key)

response = client.chat.completions.create(
    model='gpt-4o',
    temperature=0.1, # ììœ ë„
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "2026ë…„ ì›”ë“œì»µ ê°œìµœ êµ­ê°€ëŠ” ì–´ë””ì•¼?"},
    ]
)

print(response)
print('------------------')
print(response.choices[0].message.content)
```

- multi_exam.ipynb

```python
from openai import OpenAI
from http import client
from dotenv import load_dotenv
import os

load_dotenv()

api_key = os.getenv('OPENAI_API_KEY')

client = OpenAI(api_key=api_key)

def get_ai_response(messages):
    response = client.chat.completions.create(
        model='gpt-4o-mini',
        temperature=0.9, # ììœ ë„
        messages=messages,
    )
    return response.choices[0].message.content

messages=[
    {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."},
]
while True:
    user_input = input('ì‚¬ìš©ì: ')
    
    if user_input == 'exit':
        break
    
    messages.append({"role": "user", "content": user_input})
    ai_response = get_ai_response(messages)
    messages.append({"role": "assistant", "content": ai_response})
    print('AI: ' + ai_response)
```

- summary.ipynb
```
from openai import OpenAI
from http import client
from dotenv import load_dotenv
import os

load_dotenv()

api_key = os.getenv('OPENAI_API_KEY')

client = OpenAI(api_key=api_key)

def summarize_txt(file_path: str):
    client = OpenAI(api_key=api_key)
    
    with open(file_path, 'r', encoding='utf-8') as f:
        txt = f.read()
        
    # ìš”ì•½ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±
    system_prompt = f'''
    ë„ˆëŠ” ë‹¤ìŒ ê¸€ì„ ìš”ì•½í•˜ëŠ” ë´‡ì´ë‹¤. ì•„ë˜ ê¸€ì„ ì½ê³ , ì €ìì˜ ë¬¸ì œ ì¸ì‹ê³¼ ì£¼ì¥ì„ íŒŒì•…í•˜ê³ , ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ë¼.
    
    ì‘ì„±í•´ì•¼ í•˜ëŠ” í¬ë§·ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
    
    # ì œëª©
    
    ## ì €ìì˜ ë¬¸ì œ ì¸ì‹ ë° ì£¼ì¥ (15ë¬¸ì¥ ì´ë‚´)
    
    ## ì €ì ì†Œê°œ
    
    ====================================== ì´í•˜ í…ìŠ¤íŠ¸ =====================================
    {txt}
    '''
    
    print(system_prompt)
    print('===================================================================')
    
    response = client.chat.completions.create(
        model='gpt-4o-mini',
        temperature=0.1, # ììœ ë„
        messages=[
            {"role": "system", "content": system_prompt}
        ],
    )
    
    return response.choices[0].message.content
    
    if __name__ == '__main__':
    file_path = './output/ê³¼ì •ê¸°ë°˜ ì‘ë¬¼ëª¨í˜•ì„ ì´ìš©í•œ ì›¹ ê¸°ë°˜ ë°€ ì¬ë°°ê´€ë¦¬ ì˜ì‚¬ê²°ì • ì§€ì›ì‹œìŠ¤í…œ ì„¤ê³„ ë° êµ¬ì¶•_with_preprocessing.txt'
    
    summary = summarize_txt(file_path)
    print(summary)
    
    with open('./output/summary.txt', 'w', encoding='utf-8') as f:
        f.write(summary)
```
