- 4_퍼셉트론.pdf


XOR → 다중 퍼셉트론

- 5_MLP 다층퍼셉트론.pdf

https://playground.tensorflow.org/#activation=relu&regularization=L1&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=7,3&seed=0.51925&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=true&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false

- 2_ai_exam.ipynb

Adam 써라


## ✅ 1. Activation Function (활성화 함수)

### 개념

뉴런에서 **입력의 가중합을 출력으로 바꾸는 함수**입니다.

비선형성을 주입하여, 신경망이 복잡한 패턴을 학습할 수 있게 해줍니다.

### 대표적인 함수들

| 함수 이름 | 수식 | 특징 |
| --- | --- | --- |
| **Sigmoid** | 1 / (1 + exp(-x)) | 0~1 출력. 기울기 소실 문제 있음 |
| **Tanh** | (exp(x) - exp(-x)) / ... | -1~1 출력. Sigmoid보다 좋음 |
| **ReLU** | max(0, x) | 현재 가장 많이 사용됨. 간단함 |
| **Leaky ReLU** | x if x>0 else 0.01x | ReLU의 단점 보완 |
| **Softmax** | exp(xᵢ)/Σexp(xⱼ) | 분류 문제의 출력층에 사용 |

---

## ✅ 2. Optimizer (최적화 기법)

### 개념

모델의 손실(loss)을 줄이기 위해 **가중치/편향을 어떻게 업데이트할지 결정**하는 알고리즘입니다.

### 주요 Optimizer 종류

| Optimizer | 특징 |
| --- | --- |
| **SGD** | 가장 기본적인 경사하강법. 단순하지만 느림 |
| **Momentum** | 이전 업데이트 방향을 고려하여 가속함 |
| **RMSProp** | 학습률을 각 파라미터에 따라 조정함 |
| **Adam** | Momentum + RMSProp. 가장 널리 사용됨 |
| **Adagrad** | 자주 등장하는 파라미터에 작은 학습률 적용 |

---

## 🔁 정리

| 역할 | 이름 | 예시 |
| --- | --- | --- |
| 비선형 함수 (뉴런에서 출력값 계산) | Activation | `sigmoid`, `tanh`, `ReLU`, `softmax` |
| 가중치 업데이트 전략 | Optimizer | `SGD`, `Adam`, `RMSProp`, `Adagrad` |

- MLP와 케라스 라이브러리.pdf
- 심층신경망.pdf


`verbose`는 **"얼마나 자세하게 메시지를 출력할 것인가"**를 조절하는 매개변수입니다.

주로 **모델 훈련, 예측, 변환 등의 과정을 출력할 때** 사용됩니다.

의미 정리

| 값 | 의미 |
| --- | --- |
| `verbose=0` | **출력 없음** (침묵 모드) |
| `verbose=1` | **진행 상황을 한 줄씩 출력** (보통 진행바 포함) |
| `verbose=2` | **에포크별 로그만 출력** (진행바 없이 간단하게) |
