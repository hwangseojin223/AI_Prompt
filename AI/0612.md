- config.py
```python
answer_examples = [
  {
    "input": "소득은 어떻게 구분되나요?",
    "answer": """소득세법 제 4조(소득의 구분)에 따르면 소득은 아래와 같이 구분됩니다.
    
    1. 종합소득
      - 이 법에 따라 과세되는 모든 소득에서 제2호 및 제3호에 따른 소득을 제외한 소득으로서 다음 각 목의 소득을 합산한 것
      - 가. 이자소득
      - 나. 배당소득
      - 다. 사업소득
      - 라. 근로소득
      - 마. 연금소득
      - 바. 기타소득
      
    2. 퇴직소득
    3. 양도소득   
    """
  },
  {
    "input": "소득세의 과세 기간은 어떻게 되나요?",
    "answer": """소득세법 제5조(과세기간)에 따르면,
    일반적인 소득세의 과세기간은 1월 1일부터 12월 31일까지 1년 입니다.
    하지만 거주자가 사망한 경우는 1월 1일부터 사망일까지,
    거주자가 해외로 이주한 경우 1월 1일부터 출국한 날까지 입니다.    
    """
  },
  {
    "input": "원천징수 영수증은 언제 발급을 받을 수 있나요?",
    "answer": """소득세법 제123조(근로소득에 대한 원천징수영수증의 발급)에 따르면,
    근로소득을 지급하는 원천징수의무자는 해당 과세기간의 다음 연도 2월 말일까지 원천징수영수증을 근로소득자에게 발급해야하고,
    다만, 해당 과세기간 중도에 사람에게는 퇴직한 한 날의 다음 달 말일까지 발급하여야 하며,
    일용근로자에 대하여는 근로소득의 지급일이 속하는 달의 다음 달 말일까지 발급하여야 합니다.    
    """
  }
]
```

- llm.py

```python
from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from pinecone import Pinecone
from langchain_pinecone import PineconeVectorStore
from langchain import hub
from langchain.chains import RetrievalQA
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import MessagesPlaceholder
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate
from config import answer_examples

# 사용할 벡터 인덱스 이름 정의
index_name = 'tax-index'

# 세션별 대화 기록을 저장할 딕셔너리 생성
store = {}

# 세션 ID를 기반으로 대화 히스토리를 가져오거나 새로 생성
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# 사전 기반 질문 수정 체인 구성 함수
def get_dictionary_chain():
    dictionary = ['사람을 나타내는 표현 -> 거주자']
    llm = get_llm()

    prompt = ChatPromptTemplate.from_template(
        f"""
        사용자의 질문을 보고, 우리의 사전을 참고해서 사용자의 질문을 변경해 주세요.
        만약 변경할 필요가 없다고 판단 된다면, 사용자의 질문을 변경하지 않아도 됩니다.
        그럼 경우에는 질문만 리턴해 주세요.
        사전: {dictionary}
        질문: {{question}}
        """
    )

    dictionary_chain = prompt | llm | StrOutputParser()
    
    return dictionary_chain

def get_llm(model='gpt-4o'):
    llm = ChatOpenAI(model=model)
    
    return llm

def get_retriever():
    embedding = OpenAIEmbeddings(model='text-embedding-3-large') # 3072

    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=index_name,
        embedding=embedding
    )

    
    retriever = vectorstore.as_retriever(search_kwargs={'k':4})
    
    return retriever
    

def get_history_retriever():
    llm = get_llm()
    retriever = get_retriever()

    # 대화 히스토리를 고려해 질문을 재구성하는 시스템 프롬프트
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )

    # 대화 기반 질문 재구성 프롬프트 템플릿
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    
    # 히스토리 인지형 retriever 생성
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    
    return history_aware_retriever

def get_rag_chain():
    llm = get_llm()
    
    example_prompt = ChatPromptTemplate.from_messages(
        [
            ("human", "{input}"),
            ("ai", "{answer}"),
        ]
    )
    few_shot_prompt = FewShotChatMessagePromptTemplate(
        example_prompt=example_prompt,
        examples=answer_examples,
    )
    
    history_aware_retriever = get_history_retriever()
    
    # 최종 응답 생성을 위한 QA 시스템 프롬프트
    system_prompt = (
        "당신은 소득세법 전문가입니다. 사용자의 소득세법에 관한 질문에 답변해 주세요."
        "아래에 제공된 문서를 활용해서 답변해 주시고"
        "답변을 알 수 없다면 모른다고 답변해 주세요"
        "답변을 제공할 때는 소득세법 (XX조)에 따르면 이라고 시작하면서 답변해 주시고 "
        "2~3 문장 정도의 짧은 내용의 답변을 원합니다."
        "\n\n"
        "{context}"
    )
    
    # QA 프롬프트 템플릿 정의
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            few_shot_prompt,
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    
    # 검색된 문서를 기반으로 응답 생성 체인 생성
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    
    # retriever + QA chain 연결 → RAG 체인 생성
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    # 세션별로 대화 이력을 관리할 수 있도록 RAG 체인 래핑
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer",
    ).pick('answer') 

    
    return conversational_rag_chain

# 전체 파이프라인 실행 함수
def get_ai_message(user_message):
    dictionary_chain = get_dictionary_chain()  # 사전 기반 질문 변환
    rag_chain = get_rag_chain()  # RAG 체인 불러오기
    
    # 사전 처리 체인과 RAG 체인을 연결
    tax_chain = {'input': dictionary_chain} | rag_chain 

    # 최종 체인 실행 (세션 ID 지정하여 대화 이력 저장 가능)
    ai_message = tax_chain.stream(
        {
            'question': user_message
        },
        config={
            "configurable": {"session_id": "abc123"}
        },
    )
    
    return ai_message  # 생성된 응답 스트림 반환

```

- [app.py]

```python
import os
import streamlit as st 

from dotenv import load_dotenv
from llm import get_ai_message

st.set_page_config(
    page_title='소득세 챗봇',
    page_icon=':guardsman:',
    # layout='wide',
    # initial_sidebar_state='expanded'
)

st.title('Streamlit 기본 예제')
st.caption('소득세에 관련된 모든 것을 답변해 드립니다.')

load_dotenv()

if 'message_list' not in st.session_state:
    st.session_state.message_list = []

for message in st.session_state.message_list:
    with st.chat_message(message['role']):
        st.write(message['content'])
        
        
if user_question := st.chat_input(placeholder='소득세에 관련 궁금한 내용을 말씀해 주세요.'):
    with st.chat_message('user'):
        st.write(user_question)
    st.session_state.message_list.append({'role':'user', 'content':user_question})
    
    with st.spinner('답변을 생성하는 중입니다.'):
        ai_response = get_ai_message(user_question)
        
        with st.chat_message('ai'):
            ai_message = st.write_stream(ai_response)
        st.session_state.message_list.append({'role':'ai', 'content':ai_message})

```


### langgraph

## 🧠 LangGraph란?

> "LangGraph는 에이전트, 멀티턴 대화, 분기, 루프 등 복잡한 LLM 애플리케이션의 흐름을 그래프 형태로 모델링하는 프레임워크입니다."
> 

### 📌 핵심 개념

- `LangChain`과 통합됨 (`langchain`의 구성 요소를 사용)
- 워크플로우를 **노드(Node)**와 **엣지(Edge)**로 구성
- 노드: 단일 작업 (예: LLM 호출, 툴 실행)
- 엣지: 다음 노드 결정 (조건 분기, 반복 포함)
- 유사한 개념: `state machines`, `workflow engines`, `DAG`, `decision trees`

---

## 📦 주요 기능

| 기능 | 설명 |
| --- | --- |
| **Stateful Execution** | 상태 정보를 기억하며 멀티턴 대화 또는 반복 가능한 작업 가능 |
| **Branching / Looping** | 조건부 분기 및 루프를 지원하여 복잡한 워크플로우 구현 가능 |
| **Tool 사용 통합** | 외부 API 또는 함수 호출 등을 Tool로 래핑하여 처리 가능 |
| **에이전트 개발 지원** | LangChain의 Agent 구성요소와 통합하여 Agent 구축 가능 |
| **OpenAI Function Call 지원** | function-calling 또는 tool 사용과 결합한 그래프 설계 지원 |



### ToolCalling

## 🧠 Tool Calling이란?

> LLM이 자연어로 지시를 받아, 적절한 도구(Tool 또는 Function)를 직접 선택하고 호출하는 기능입니다.
> 

예를 들어 사용자가:

> "오늘 서울 날씨 알려줘."
> 

라고 입력하면, LLM은 스스로 "날씨 검색 API"를 호출하고, 그 결과를 사용자에게 보여줍니다.

- langgraph_rag.pdf

https://www.tavily.com/

workspace/ai_agent_work2/langgraph_env



- 01_langgraph_exam.ipynb
